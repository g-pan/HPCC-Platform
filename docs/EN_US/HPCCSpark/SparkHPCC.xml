<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<book xml:base="../">
  <bookinfo>
    <title>HPCC / Spark Connector</title>

    <mediaobject>
      <imageobject>
        <imagedata fileref="images/redswooshWithLogo3.jpg" />
      </imageobject>
    </mediaobject>

    <author>
      <surname>Boca Raton Documentation Team</surname>
    </author>

    <legalnotice>
      <para>We welcome your comments and feedback about this document via
      email to <email>docfeedback@hpccsystems.com</email></para>

      <para>Please include <emphasis role="bold">Documentation
      Feedback</emphasis> in the subject line and reference the document name,
      page numbers, and current Version Number in the text of the
      message.</para>

      <para>LexisNexis and the Knowledge Burst logo are registered trademarks
      of Reed Elsevier Properties Inc., used under license.</para>

      <para>HPCC Systems<superscript>®</superscript> is a registered trademark
      of LexisNexis Risk Data Management Inc.</para>

      <para>Other products, logos, and services may be trademarks or
      registered trademarks of their respective companies.</para>

      <para>All names and example data used in this manual are fictitious. Any
      similarity to actual persons, living or dead, is purely
      coincidental.</para>

      <para></para>
    </legalnotice>

    <xi:include href="common/Version.xml" xpointer="FooterInfo"
                xmlns:xi="http://www.w3.org/2001/XInclude" />

    <xi:include href="common/Version.xml" xpointer="DateVer"
                xmlns:xi="http://www.w3.org/2001/XInclude" />

    <corpname>HPCC Systems<superscript>®</superscript></corpname>

    <xi:include href="common/Version.xml" xpointer="Copyright"
                xmlns:xi="http://www.w3.org/2001/XInclude" />

    <mediaobject role="logo">
      <imageobject>
        <imagedata fileref="images/LN_Rightjustified.jpg" />
      </imageobject>
    </mediaobject>
  </bookinfo>

  <chapter>
    <title>The Spark HPCC Systems Connector</title>

    <sect1 id="overview" role="nobrk">
      <title>Overview</title>

      <para>The Spark-HPCCSystems Distributed Connector is a Java library that
      facilitates access from a Spark cluster to data stored on an HPCC
      Systems cluster. The connector library employs the standard HPCC Systems
      remote file read facility to read data from either sequential or indexed
      HPCC datasets.</para>

      <para>The data on an HPCC cluster is partitioned horizontally, with data
      on each cluster node. Once configured, the HPCC data is available for
      reading in parallel by the Spark cluster.</para>

      <para>In the GitHub repository (<ulink
      url="https://github.com/hpcc-systems/Spark-HPCC">https://github.com/hpcc-systems/Spark-HPCC</ulink>)
      you can find the source code and examples. There are several artifacts
      in the DataAccess/src/main/java folder of primary interest. The
      <emphasis>org.hpccsystems.spark.HpccFile</emphasis> class is the façade
      of a file on an HPCC Cluster. The
      <emphasis>org.hpccsystems.spark.HpccRDD</emphasis> is a resilient
      distributed dataset derived from the data on the HPCC Cluster and is
      created by the
      <emphasis>org.hpccsystems.spark.HpccFile.getRDD</emphasis>(…) method.
      The <emphasis>HpccFile</emphasis> class supports loading data to
      construct a <emphasis>Dataset&lt;Row&gt;</emphasis> object for the Spark
      interface. This will first load the data into an RDD&lt;Row&gt; and then
      convert this RDD to a Dataset&lt;Row&gt; through internal Spark
      mechanisms.</para>

      <para>There are several additional artifacts of some interest. The
      <emphasis>org.hpccsystems.spark.ColumnPruner</emphasis> class is
      provided to enable retrieving only the columns of interest from the HPCC
      Cluster. The <emphasis>targetCluster</emphasis> artifact allows you to
      specify the HPCC cluster on which the target file exists. The
      <emphasis>org.hpccsystems.spark.thor.FileFilter</emphasis> class is
      provided to facilitate filtering records of interest from the HPCC
      Cluster.</para>

      <para>The git repository includes two examples under the
      Examples/src/main/scala folder. The examples
      (<emphasis>org.hpccsystems.spark_examples.Dataframe_Iris_LR</emphasis>
      and <emphasis>org.hpccsystems.spark_examples.Iris_LR</emphasis>) are
      Scala Objects with a main() function. Both examples use the classic Iris
      dataset. The dataset can be obtained from a variety of sources,
      including the HPCC-Systems/ecl-ml repository. IrisDs.ecl (can be found
      under the ML/Tests/Explanatory folder: <ulink
      url="https://github.com/hpcc-systems/Spark-HPCC/blob/master/Examples/src/main/ecl/IrisDS.ecl">https://github.com/hpcc-systems/Spark-HPCC/blob/master/Examples/src/main/ecl/IrisDS.ecl</ulink>)
      can be executed to generate the Iris dataset in HPCC. A walk-through of
      the examples is provided in the Examples section.</para>

      <para>The Spark-HPCCSystems Distributed Connector also supports PySpark.
      It uses the same classes/API as Java does.</para>

      <para><informaltable colsep="1" frame="all" rowsep="1">
          <?dbfo keep-together="always"?>

          <tgroup cols="2">
            <colspec colwidth="49.50pt" />

            <colspec />

            <tbody>
              <row>
                <entry><graphic fileref="images/tip.jpg" /></entry>

                <entry><para>As is common in Java client communication over
                TLS, Spark-HPCC connectors targeting an HPCC cluster over TLS
                will need to import the appropriate certificates to local Java
                keystore.</para><para> </para><para>*One way to accomplish
                this is to use the keytool packaged with Java installations.
                Refer to the keytool documentation for usage. </para></entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable></para>

      <sect2>
        <title>Spark Integration</title>

        <para>The HPCC integrated Spark plugin is no longer supported as of
        version 9.0.0 in favor of stand-alone user-managed Spark clusters
        linked to the HPCC platform using the Spark-HPCC connector.</para>
      </sect2>

      <sect2 role="brk">
        <title>Special considerations</title>

        <sect3>
          <title>Unsigned Value Overflow</title>

          <para>Java does not support an unsigned integer type so reading
          UNSIGNED8 values from HPCC data can cause an integer overflow in
          Java. UNSIGNED8 values are often used as unique identifiers in
          datasets, in which case overflowing would be acceptable as the
          overflowed value will still be unique.</para>

          <para>The Spark-HPCC connector allows unsigned values to overflow in
          Java and will not report a exception. The caller is responsible for
          interpreting the value based on the recdef <emphasis
          role="bold">isunsigned</emphasis> flag.</para>
        </sect3>
      </sect2>
    </sect1>

    <sect1 id="primary-classes">
      <title>Primary Classes</title>

      <para>The <emphasis>HpccFile</emphasis> class and the
      <emphasis>HpccRDD</emphasis> classes are discussed in more detail below.
      These are the primary classes used to access data from an HPCC Cluster.
      The <emphasis>HpccFile</emphasis> class supports loading data to
      construct a <emphasis>Dataset&lt;Row&gt;</emphasis> object for the Spark
      interface. This will first load the data into an RDD&lt;Row&gt; and then
      convert this RDD to a Dataset&lt;Row&gt; through internal Spark
      mechanisms.</para>

      <para>The <emphasis>org.hpccsystems.spark.HpccFile</emphasis> class has
      several constructors. All of the constructors take information about the
      Cluster and the name of the dataset of interest. The JAPI WS-Client
      classes are used to access file detail information. A definition used to
      select the columns to be returned and a definition to select the rows to
      be returned could also be supplied. These are discussed in the
      <emphasis>Additional Classes of Interest</emphasis> section below. The
      class has two methods of primary interest: the
      <emphasis>getRDD(…)</emphasis> method and the
      <emphasis>getDataframe(…)</emphasis> method, which are illustrated in
      the <emphasis>Example</emphasis> section.</para>

      <para>The <emphasis>HpccFile</emphasis> class
      <emphasis>getRecordDefinition()</emphasis> method can be used to
      retrieve a definition of the file. The
      <emphasis>getFileParts()</emphasis> method can be used to see how the
      file is partitioned on the HPCC Cluster. These methods return the same
      information as can be found on the ECL Watch dataset details page DEF
      tab and the PARTS tab respectively.</para>

      <para>The <emphasis>org.hpccsystems.spark.HpccRDD</emphasis> class
      extends the <emphasis>RDD&lt;Record&gt;</emphasis> templated class. The
      class employs the <emphasis>org.hpccsystems.spark.HpccPart</emphasis>
      class for the Spark partitions. The
      <emphasis>org.hpccsystems.spark.Record</emphasis> class is used as the
      container for the fields from the HPCC Cluster. The
      <emphasis>Record</emphasis> class can create a <emphasis>Row</emphasis>
      instance with a schema.</para>

      <para>The <emphasis>HpccRDD</emphasis> HpccPart partition objects each
      read blocks of data from the HPCC Cluster independently from each other.
      The initial read fetches the first block of data, requests the second
      block of data, and returns the first record. When the block is
      exhausted, the next block should be available on the socket and new read
      request is issued.</para>

      <para>The <emphasis>HpccFileWriter</emphasis> is another primary class
      used for writing data to an HPCC Cluster. It has a single constructor
      with the following signature:</para>

      <programlisting>public HpccFileWriter(String connectionString, String user, String pass) throws Exception { </programlisting>

      <para>The first parameter <emphasis>connectionString</emphasis> contains
      the same information as <emphasis>HpccFile</emphasis>. It should be in
      the following format:
      {http|https}://{ECLWATCHHOST}:{ECLWATCHPORT}</para>

      <para>The constructor will attempt to connect to HPCC. This connection
      will then be used for any subsequent calls to
      <emphasis>saveToHPCC</emphasis>.</para>

      <programlisting>public long saveToHPCC(SparkContext sc, RDD&lt;Row&gt; scalaRDD, String clusterName, 
                        String fileName) throws Exception {</programlisting>

      <para>The <emphasis>saveToHPCC</emphasis> method only supports
      RDD&lt;row&gt; types. You may need to modify your data representation to
      use this functionality. However, this data representation is what is
      used by Spark SQL and by HPCC. This is only supported by writing in a
      co-located setup. Thus Spark and HPCC must be installed on the same
      nodes. Reading only supports reading data in from a remote HPCC
      cluster.</para>

      <para>The <emphasis>clusterName</emphasis> as used in the above case is
      the desired cluster to write data to, for example, the "mythor" Thor
      cluster. Currently there is only support for writing to Thor clusters.
      Writing to a Roxie cluster is not supported and will return an
      exception. The filename as used in the above example is in the HPCC
      format, for example: "~example::text".</para>

      <para>Internally the saveToHPCC method will Spawn multiple Spark jobs.
      Currently, this spawns two jobs. The first job maps the location of
      partitions in the Spark cluster so it can provide this information to
      HPCC. The second job does the actual writing of files. There are also
      some calls internally to ESP to handle things like starting the writing
      process by calling <emphasis>DFUCreateFile</emphasis> and publishing the
      file once it has been written by calling
      <emphasis>DFUPublishFile</emphasis>.</para>

      <sect2 role="brk">
        <title>Using the Spark Datasource API to Read and Write</title>

        <para>Example Python code:</para>

        <para><programlisting># Connect to HPCC and read a file
df = spark.read.load(format="hpcc",
                     host="127.0.0.1:8010",
                     password="",
                     username="",
                     limitPerFilePart=100,        
                                      # Limit the number of rows to read from each file part
                     projectList="field1, field2, field3.childField1",     
                                      # Comma separated list of columns to read
                     fileAccessTimeout=240,
                     path="example::file")
# Write the file back to HPCC
df.write.save(format="hpcc",
              mode="overwrite",        
                   # Left blank or not specified results in an error if the file exists
              host="127.0.0.1:8010",
              password="",
              username="",
              cluster="mythor",
              path="example::file")</programlisting></para>

        <para>Example Scala code:</para>

        <para><programlisting>// Read a file from HPCC
val dataframe = spark.read.format("hpcc")
                .option("host","127.0.0.1:8010")
                .option("password", "")
                .option("username", "")
                .option("limitPerFilePart",100)
                .option("fileAccessTimeout",240)
                .option("projectList","field1, field2, field3.childField")
                .load("example::file")
// Write the dataset back
   dataframe.write.mode("overwrite")
                   .format("hpcc")
                   .option("host","127.0.0.1:8010")
                   .option("password", "")
                   .option("username", "")
                   .option("cluster","mythor")
                   .save("example::file")</programlisting></para>

        <para>Example R code:</para>

        <para><programlisting>df &lt;- read.df(source = "hpcc",
              host = "127.0.0.1:8010",
              path = "example::file",
              password = "",
              username = "",
              limitPerFilePart = 100,
              fileAccessTimeout = 240,
              projectList = "field1, field2, field3.childField")
write.df(df, source = "hpcc",
             host = "127.0.0.1:8010",
             cluster = "mythor",
             path = "example::file",
             mode = "overwrite",
             password = "",
             username = "",
             fileAccessTimeout = 240)</programlisting></para>
      </sect2>
    </sect1>

    <sect1 id="additional-classes-of-interest">
      <title>Additional Classes of interest</title>

      <para>The main classes of interest for this section are column pruning
      and file filtering. In addition there is a helper class to remap IP
      information when required, and this is also discussed below.</para>

      <para>The column selection information is provided as a string to the
      <emphasis>org.hpccsystems.spark.ColumnPruner</emphasis> object. The
      string is a list of comma separated field names. A field of interest
      could contain a row or child dataset, and the dotted name notation is
      used to support the selection of individual child fields. The
      <emphasis>ColumnPruner</emphasis> parses the string into a root
      <emphasis>TargetColumn</emphasis> class instance which holds the top
      level target columns. A <emphasis>TargetColumn</emphasis> can be a
      simple field or can be a child dataset and so be a root object for the
      child record layout.</para>

      <para>The row filter is implemented in the
      <emphasis>org.hpccsystems.spark.thor.FileFilter</emphasis> class. A
      <emphasis>FileFilter</emphasis> instance is constricted from an array of
      <emphasis>org.hpccsystems.spark.thor.FieldFilter</emphasis> objects.
      Each <emphasis>FieldFilter</emphasis> instance is composed of a field
      name (in doted notation for compound names) and an array of
      <emphasis>org.hpccsystems.spark.thor.FieldFilterRange</emphasis>
      objects. Each <emphasis>FieldFilterRange</emphasis> instance can be an
      open or closed interval or a single value. The record is selected when
      at least one <emphasis>FieldFilterRange</emphasis> matches for each of
      the <emphasis>FieldFilter</emphasis> instances in the array.</para>

      <para>The <emphasis>FieldFilterRange</emphasis> values may be either
      strings or numbers. There are methods provided to construct the
      following range tests: equals, not equals, less than, less than or
      equals, greater than, and a greater than or equals. In addition, a set
      inclusion test is supported for strings. If the file is an index, the
      filter fields that are key fields are used for an index lookup. Any
      filter field unmentioned is treated as wild.</para>

      <para>The usual deployment architecture for HPCC Clusters consists of a
      collection of nodes on a network. The file management information
      includes the IP addresses of the nodes that hold the partitions of the
      file. The Spark-HPCC connector classes use these IP addresses to
      establish socket connections for the remote read. An HPCC Cluster may be
      deployed as a virtual cluster with private IP addresses. This works for
      the cluster components because they are all on the same private LAN.
      However, the Spark cluster nodes might not be on that same LAN. In this
      case, the <emphasis>org.hpccsystems.spark.RemapInfo</emphasis> class is
      used to define the information needed to change the addressing. There
      are two options that can be used. The first option is that each Thor
      worker node can be assigned an IP that is visible to the Spark cluster.
      These addresses must be a contiguous range. The second option is to
      assign an IP and a contiguous range of port numbers. The
      <emphasis>RemapInfo</emphasis> object is supplied as a parameter.</para>
    </sect1>

    <sect1 id="examples">
      <title>Examples</title>

      <para>We will walk through the two examples below utilizing a Spark
      environment. Additionally, the repository provides testing programs (in
      the DataAccess/src/test folder) that can be executed as stand-alone
      examples.  </para>

      <para>These test programs are intended to be run from a development IDE
      such as Eclipse via the Spark-submit application whereas the examples
      below are dependent on the Spark shell.</para>

      <para>The following examples assume a Spark Shell. You can use the
      spark-submit command if you intend to compile and package these
      examples. To properly connect your spark shell to the Integrated Spark
      cluster, provide the following parameters when starting the
      shell:</para>

      <programlisting>bin/spark-shell  \
 --master=&lt;spark://{remotesparkhost-IP}:{sparkport}&gt; --conf="spark.driver.host={localhost-ip}" </programlisting>

      <sect2 id="iris_lr">
        <title>Iris_LR</title>

        <para>This example assumes that you have Spark Shell running. The next
        step is to establish your HpccFile and your RDD for that file. You
        need the name of the file, the protocol (http or https), the name or
        IP of the ESP, the port for the ESP (usually 8010), and your user
        account and password. The <emphasis>sc</emphasis> value is the
        <emphasis>SparkContext</emphasis> object provided by the shell.</para>

        <programlisting> val espcon = new Connection("http", "myeclwatchhost", "8010");
 espcon.setUserName("myuser");
 espcon.setPassword("mypass");
 val file = new HpccFile("myfile",espcon);
</programlisting>

        <para>Now we have an RDD of the data. Nothing has actually happened at
        this point because Spark performs lazy evaluation and there is nothing
        yet to trigger an evaluation.</para>

        <para>The Spark MLLib has a Logistic Regression package. The MLLib
        Logistic Regression expects that the data will be provided as Labeled
        Point formatted records. This is common for supervised training
        implementations in MLLib. We need column labels, so we make an array
        of names. We then make a labeled point RDD from our RDD. This is also
        just a definition. Finally, we define the Logistic Regression that we
        want to run. The column names are the field names in the ECL record
        definition for the file, including the name “class” which is the name
        of the field holding the classification code.</para>

        <programlisting> val names = Array("petal_length","petal_width", "sepal_length", 
                   "sepal_width")
 var lpRDD = myRDD.makeMLLibLabeledPoint("class", names)
 val lr = new LogisticRegressionWithLBFGS().setNumClasses(3)</programlisting>

        <para>The next step is to define the model, which is an action and
        will cause Spark to evaluate the definitions.</para>

        <programlisting> val iris_model = lr.run(lpRDD)</programlisting>

        <para>Now we have a model. We will utilize this model to take our
        original dataset and use the model to produce new labels. The correct
        way to do this is to have randomly sampled some hold out data. We are
        just going to use the original dataset because it is easier to show
        how to use the connector. We then take our original data and use a map
        function defined inline to create a new record with our prediction
        value and the original classification.</para>

        <programlisting> val predictionAndLabel = lpRDD.map {
   case LabeledPoint(label, features) =&gt;
     val prediction = iris_model.predict(features)
   (prediction, label)
 }</programlisting>

        <para>The <emphasis>MulticlassMetrics</emphasis> class can now be used
        to produce a confusion matrix.</para>

        <programlisting> val metrics = new MulticlassMetrics(predictionAndLabel)
 metrics.confusionMatrix</programlisting>
      </sect2>

      <sect2 id="dataframe_iris_lr">
        <title>Dataframe_Iris_LR</title>

        <para>The Dataframe_Iris_LR is similar to the Iris_LR, except that a
        Dataframe is used and the new ML Spark classes are used instead of the
        old MLLib classes. Since ML is not completely done, we do fall back to
        an MLLib class to create our confusion matrix.</para>

        <para>Once the Spark shell is brought up, we need our import
        classes.</para>

        <para><programlisting> import org.hpccsystems.spark.HpccFile
 import org.apache.spark.sql.Dataset
 import org.apache.spark.ml.feature.VectorAssembler
 import org.apache.spark.ml.classification.LogisticRegression
 import org.apache.spark.mllib.evaluation.MulticlassMetrics</programlisting></para>

        <para>The next step is to establish the <emphasis>HpccFile</emphasis>
        object and create the Dataframe. The <emphasis>spark</emphasis> value
        is a <emphasis>SparkSession</emphasis> object supplied by the shell
        and is used instead of the <emphasis>SparkContext</emphasis>
        object.</para>

        <programlisting> val espcon = new Connection("http", "myeclwatchhost", "8010");
 espcon.setUserName("myuser");
 espcon.setPassword("mypass");
 val file = new HpccFile("myfile",espcon);
</programlisting>

        <para>The Spark <emphasis>ml</emphasis> Machine Learning classes use
        different data container classes. In the case of Logistic Regression,
        we need to transform our data rows into a row with a column named
        “features” holding the features and a column named “label” holding the
        classification label. Recall that our row has “class”, “sepal_width”,
        “sepal_length”, “petal_width”, and “petal_length” as the column names.
        This kind of transformation can be accomplished with a
        <emphasis>VectorAssembler</emphasis> class.</para>

        <programlisting> val assembler = new VectorAssembler()
 assembler.setInputCols(Array("petal_length","petal_width",
                              "sepal_length", "sepal_width"))
 assembler.setOutputCol("features")
 val iris_fv = assembler.transform(my_df)
                              .withColumnRenamed("class", "label")</programlisting>

        <para>Now that the data (<emphasis>iris_fv</emphasis>) is ready, we
        define our model and fit the data.</para>

        <programlisting> val lr = new LogisticRegression()
 val iris_model = lr.fit(iris_fv)</programlisting>

        <para>We now want to apply our prediction and evaluate the results. As
        noted before, we would use a holdout dataset to perform the
        evaluation. We are going to be lazy and just use the original data to
        avoid the sampling task. We use the <emphasis>transform(…)</emphasis>
        function for the model to add the prediction. The function adds a
        column named “prediction” and defines a new dataset. The new Machine
        Learning implementation lacks a metrics capability to produce a
        confusion matrix, so we will then take our dataset with the
        <emphasis>prediction</emphasis> column and create a new RDD with a
        dataset for a <emphasis>MulticlassMetrics</emphasis> class.</para>

        <programlisting> val with_preds = iris_model.transform(iris_fv)
 val predictionAndLabel = with_preds.rdd.map(
 r =&gt; (r.getDouble(r.fieldIndex("prediction")),
 r.getDouble(r.fieldIndex("label"))))
 val metrics = new MulticlassMetrics(predictionAndLabel)
 metrics.confusionMatrix</programlisting>
      </sect2>
    </sect1>
  </chapter>
</book>
